{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA-qGtr_lJWX",
        "outputId": "bc51b87c-8c85-4ce5-87ca-3e35186cc5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m818.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.11.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.43.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cxRNxqMMWOF",
        "outputId": "f4eeb01e-5a64-4d8b-cf9e-dde56a1ffd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openpifpaf==0.13.11\n",
            "  Downloading openpifpaf-0.13.11.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.3/202.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata!=3.8.0 in /usr/local/lib/python3.10/dist-packages (from openpifpaf==0.13.11) (7.1.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from openpifpaf==0.13.11) (1.25.2)\n",
            "Collecting pysparkling (from openpifpaf==0.13.11)\n",
            "  Downloading pysparkling-0.6.2.tar.gz (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.2/166.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-json-logger (from openpifpaf==0.13.11)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from openpifpaf==0.13.11) (1.13.1)\n",
            "Collecting torchvision==0.14.1 (from openpifpaf==0.13.11)\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.10/dist-packages (from openpifpaf==0.13.11) (9.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->openpifpaf==0.13.11) (4.11.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->openpifpaf==0.13.11) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->openpifpaf==0.13.11) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->openpifpaf==0.13.11) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->openpifpaf==0.13.11) (11.7.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1->openpifpaf==0.13.11) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->openpifpaf==0.13.11) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->openpifpaf==0.13.11) (0.43.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=3.8.0->openpifpaf==0.13.11) (3.18.1)\n",
            "Requirement already satisfied: pytz>=2019.3 in /usr/local/lib/python3.10/dist-packages (from pysparkling->openpifpaf==0.13.11) (2023.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from pysparkling->openpifpaf==0.13.11) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->pysparkling->openpifpaf==0.13.11) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->openpifpaf==0.13.11) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->openpifpaf==0.13.11) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->openpifpaf==0.13.11) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->openpifpaf==0.13.11) (2024.2.2)\n",
            "Building wheels for collected packages: openpifpaf, pysparkling\n",
            "  Building wheel for openpifpaf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openpifpaf: filename=openpifpaf-0.13.11-cp310-cp310-linux_x86_64.whl size=7106902 sha256=cd3e1efcaed275d0c8377d9e31c4224fea24a3f3e09bb8f85114dc5dfebb2428\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/b8/08/07959dec67c146680770f66246d35138febd2b7d21c733174a\n",
            "  Building wheel for pysparkling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysparkling: filename=pysparkling-0.6.2-py3-none-any.whl size=185875 sha256=90db9f0a6ae21963cfc5c5e7a7baaa0a840367041f9603d9fc41fd14fafe54e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4e/9f/ebee95d389ea78f11338ba3fe358c09d047f065c4d6ac4cd88\n",
            "Successfully built openpifpaf pysparkling\n",
            "Installing collected packages: python-json-logger, pysparkling, torchvision, openpifpaf\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "Successfully installed openpifpaf-0.13.11 pysparkling-0.6.2 python-json-logger-2.0.7 torchvision-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openpifpaf==0.13.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pcd5fKmRb2To"
      },
      "outputs": [],
      "source": [
        "# import required frameworks and libraries\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "from tensorflow import keras\n",
        "from openpifpaf import Predictor\n",
        "import openpifpaf\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RljhGG3Nvqq6"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE4YdDDvxCjX"
      },
      "source": [
        "## Using Computer vision techniques to detect ongoing fire in real time\n",
        "## Using Pre-trained CNN models to detect person fall in real time\n",
        "### In our case we will use resnet101 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8YlWq2LnmiYZ"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9HQLawvZJ0Ql"
      },
      "outputs": [],
      "source": [
        "## Centroid tracker function\n",
        "### Centroid tracker to track persons for 30\n",
        "from scipy.spatial import distance as dist\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CentroidTracker:\n",
        "\n",
        "    def __init__(self, maxDisappeared=50):\n",
        "\n",
        "        # a counter to asign a uniqueID to each object\n",
        "        # if the object passes the maxDisappeared number of frames we will asign a new ObjectID\n",
        "        self.nextObjectID = 0\n",
        "\n",
        "        # a dict that stores for each object the objectID as key\n",
        "        # and the centroid(x,y) as value\n",
        "        self.objects = OrderedDict()\n",
        "\n",
        "        # a dict that stores number of consecutive frames the object has disappeared in as a key\n",
        "        # and objectID as value\n",
        "        self.disappeared = OrderedDict()\n",
        "\n",
        "        # the maximum number of frames for an object to not be in before declared disappeared\n",
        "        self.maxDisappeared = maxDisappeared\n",
        "\n",
        "    def register(self, centroid):\n",
        "\n",
        "        self.objects[self.nextObjectID] = centroid\n",
        "        self.disappeared[self.nextObjectID] = 0\n",
        "        self.nextObjectID += 1\n",
        "\n",
        "    def deregister(self, objectID):\n",
        "\n",
        "        del self.objects[objectID]\n",
        "        del self.disappeared[objectID]\n",
        "\n",
        "    def update(self, rects):\n",
        "\n",
        "        # rects format is a tuple (startX, startY, endX, endY)\n",
        "\n",
        "        # checks to see if the list of input bounding box rectangles is empty\n",
        "\n",
        "        if len(rects) == 0:\n",
        "            # no object available in the given frame\n",
        "            # Mark all existing objects as disappeared\n",
        "            for objectId in list(self.disappeared.keys()):\n",
        "                self.disappeared[objectId] += 1\n",
        "\n",
        "                # deregister objects with disappeared_Frames_Number > maxDisappeared\n",
        "                if self.disappeared[objectId] > self.maxDisappeared:\n",
        "                    self.deregister(objectId)\n",
        "\n",
        "            return self.objects\n",
        "\n",
        "        # a numpy array to store objects centroids\n",
        "        input_centroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "\n",
        "        # loop over object rectangles and compute centroids than store it in input_centroids\n",
        "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
        "\n",
        "            cX = int((startX + endX) / 2.0)\n",
        "            cY = int((startY + endY) / 2.0)\n",
        "\n",
        "            input_centroids[i] = (cX, cY)\n",
        "\n",
        "        # if there is no existing object already being tracked than we register these new objects\n",
        "        if len(self.objects) == 0:\n",
        "            for i in range(0, len(input_centroids)):\n",
        "                self.register(input_centroids[i])\n",
        "\n",
        "\n",
        "        else:\n",
        "            # grab objectID/centroid\n",
        "\n",
        "            objectIDs = list(self.objects.keys())\n",
        "            objects_centroids = list(self.objects.values())\n",
        "\n",
        "            # compute the euclidean distance between each pair of object centroids and input_centroids\n",
        "            D = dist.cdist(np.array(objects_centroids), input_centroids)\n",
        "\n",
        "            rows = D.min(axis=1).argsort()\n",
        "\n",
        "            cols = D.argmin(axis=1)[rows]\n",
        "\n",
        "            usedRows = set()\n",
        "            usedCols = set()\n",
        "\n",
        "            for (row, col) in zip(rows, cols):\n",
        "\n",
        "                # print(f\"row : {row} col : {col}\")\n",
        "                if row in usedRows or col in usedCols:\n",
        "                    continue\n",
        "\n",
        "                objectID = objectIDs[row]\n",
        "                self.objects[objectID] = input_centroids[col]\n",
        "                self.disappeared[objectID] = 0\n",
        "\n",
        "                usedRows.add(row)\n",
        "                usedCols.add(col)\n",
        "\n",
        "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "\n",
        "            # if number of existing object_centroids is equal or grater than input_centroids\n",
        "            # we need to check if some of these objects have been disappeared\n",
        "\n",
        "            if D.shape[0] >= D.shape[1]:\n",
        "                for row in unusedRows:\n",
        "\n",
        "                    objectID = objectIDs[row]\n",
        "                    self.disappeared[objectID] += 1\n",
        "\n",
        "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
        "                        self.deregister(objectID)\n",
        "\n",
        "            else:\n",
        "                # register new objects\n",
        "                for col in unusedCols:\n",
        "                    self.register(input_centroids[col])\n",
        "\n",
        "        return self.objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN3ZyqzZn6My"
      },
      "source": [
        "### Model architecture  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ead0d45f21a42168188a96d93fb0ead",
            "1b07d67684d6462398cc1fd7827f7005",
            "145f62fca5ee4d35a43d4ad507340899",
            "8a8ef8f9ee6b4ab4b052dee076a2eb90",
            "9ed9b7398c8a40b9bd47fc54fe8fbbaf",
            "452b1201d613447ba85d98d30c3857cf",
            "6823f3c012b749b2821c238fbbadc520",
            "40fe6fcf396c4b8796ccee81e64fed72",
            "4e14dd9c4eed4d7aaee2becd3d53ee20",
            "7be17be8db9a426bbaacfe49f056c8b5",
            "45c39b6433aa4ec6bd72b6b3f04df4c5"
          ]
        },
        "id": "6CIUYh5xoBqm",
        "outputId": "eaa82582-e3c8-433f-c029-f0152691c27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"http://github.com/openpifpaf/torchhub/releases/download/v0.13/shufflenetv2k16-210820-232500-cocokp-slurm726069-edge513-o10s-7189450a.pkl\" to /root/.cache/torch/hub/checkpoints/shufflenetv2k16-210820-232500-cocokp-slurm726069-edge513-o10s-7189450a.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/38.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ead0d45f21a42168188a96d93fb0ead"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Shell(\n",
              "  (base_net): ShuffleNetV2K(\n",
              "    (input_block): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (stage2): Sequential(\n",
              "      (0): InvertedResidualK(\n",
              "        (branch1): Sequential(\n",
              "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(24, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (4): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(24, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(174, 174, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=174, bias=False)\n",
              "          (4): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(174, 174, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=174, bias=False)\n",
              "          (4): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(174, 174, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=174, bias=False)\n",
              "          (4): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(174, 174, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=174, bias=False)\n",
              "          (4): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(174, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(174, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (stage3): Sequential(\n",
              "      (0): InvertedResidualK(\n",
              "        (branch1): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=348, bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (4): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(348, 348, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=348, bias=False)\n",
              "          (4): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(348, 348, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(348, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (stage4): Sequential(\n",
              "      (0): InvertedResidualK(\n",
              "        (branch1): Sequential(\n",
              "          (0): Conv2d(696, 696, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=696, bias=False)\n",
              "          (1): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (4): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(696, 696, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=696, bias=False)\n",
              "          (4): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(696, 696, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=696, bias=False)\n",
              "          (4): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(696, 696, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=696, bias=False)\n",
              "          (4): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidualK(\n",
              "        (branch2): Sequential(\n",
              "          (0): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(696, 696, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=696, bias=False)\n",
              "          (4): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (5): Conv2d(696, 696, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (6): BatchNorm2d(696, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (7): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (conv5): Sequential(\n",
              "      (0): Conv2d(1392, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1392, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (head_nets): ModuleList(\n",
              "    (0): CompositeField4(\n",
              "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
              "      (conv): Conv2d(1392, 340, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (upsample_op): PixelShuffle(upscale_factor=2)\n",
              "    )\n",
              "    (1): CompositeField4(\n",
              "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
              "      (conv): Conv2d(1392, 608, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (upsample_op): PixelShuffle(upscale_factor=2)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# initialize fall detection\n",
        "predictor = Predictor(checkpoint='shufflenetv2k16')\n",
        "annotation_painter = openpifpaf.show.AnnotationPainter()\n",
        "centroid_tracker = CentroidTracker()\n",
        "\n",
        "predictor.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TCisckcSv2Uq",
        "outputId": "bccddfaa-168e-497a-a094-1eed2628f576"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    #faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "    # detect fire\n",
        "    # Frame preprocessing; Clearing the image to later detect fire\n",
        "    # lets apply blur to reduce noise\n",
        "    imgBlurred = cv2.GaussianBlur(img, (23, 23), 0)\n",
        "\n",
        "    # Lets convert the image to HSV\n",
        "    imgHSV = cv2.cvtColor(imgBlurred, cv2.COLOR_BGR2HSV)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    # Define the mask\n",
        "    lower_mask_value = [18, 50, 50]\n",
        "    upper_mask_value = [36, 255, 255]\n",
        "\n",
        "    lower_mask_value = np.array(lower_mask_value, dtype='uint8')\n",
        "    upper_mask_value = np.array(upper_mask_value, dtype='uint8')\n",
        "\n",
        "    mask = cv2.inRange(imgHSV, lower_mask_value, upper_mask_value)\n",
        "    # Count the total number of red pixels ; total number of non zero pixels\n",
        "    total_number = cv2.countNonZero(mask)\n",
        "    bbox_array = cv2.putText(bbox_array,\n",
        "                    'Fire status:',\n",
        "                    (0, 20),\n",
        "                    font, 1,\n",
        "                    (255,0,0))\n",
        "    #print(total_number)\n",
        "    if total_number > 21000:\n",
        "\n",
        "      bbox_array = cv2.putText(bbox_array,\n",
        "                    'Fire detected',\n",
        "                    (0 , 50),\n",
        "                    font, 1,\n",
        "                    (255,0,0))\n",
        "\n",
        "    ## Detec person falling in realtime\n",
        "    #print(\"hhhhhh\")\n",
        "    predictions, gt_anns, image_meta = predictor.numpy_image(img)\n",
        "    #print(predictions)\n",
        "    person_number = len(predictions)\n",
        "    #print(person_number)\n",
        "\n",
        "    bbox_array = cv2.putText(bbox_array,\n",
        "                    'Persons detected:',\n",
        "                    (0 , 80),\n",
        "                    font, 1,\n",
        "                    (255,0,0))\n",
        "    nb = str(person_number)\n",
        "    bbox_array = cv2.putText(bbox_array,\n",
        "                    nb,\n",
        "                    (0 , 100),\n",
        "                    font, 1,\n",
        "                    (255,0,0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # get face bounding box for overlay\n",
        "    #for (x,y,w,h) in faces:\n",
        "      #bbox_array = cv2.rectangle(bbox_array,(150,150),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ead0d45f21a42168188a96d93fb0ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b07d67684d6462398cc1fd7827f7005",
              "IPY_MODEL_145f62fca5ee4d35a43d4ad507340899",
              "IPY_MODEL_8a8ef8f9ee6b4ab4b052dee076a2eb90"
            ],
            "layout": "IPY_MODEL_9ed9b7398c8a40b9bd47fc54fe8fbbaf"
          }
        },
        "1b07d67684d6462398cc1fd7827f7005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_452b1201d613447ba85d98d30c3857cf",
            "placeholder": "​",
            "style": "IPY_MODEL_6823f3c012b749b2821c238fbbadc520",
            "value": "100%"
          }
        },
        "145f62fca5ee4d35a43d4ad507340899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40fe6fcf396c4b8796ccee81e64fed72",
            "max": 40818457,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e14dd9c4eed4d7aaee2becd3d53ee20",
            "value": 40818457
          }
        },
        "8a8ef8f9ee6b4ab4b052dee076a2eb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be17be8db9a426bbaacfe49f056c8b5",
            "placeholder": "​",
            "style": "IPY_MODEL_45c39b6433aa4ec6bd72b6b3f04df4c5",
            "value": " 38.9M/38.9M [00:00&lt;00:00, 118MB/s]"
          }
        },
        "9ed9b7398c8a40b9bd47fc54fe8fbbaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "452b1201d613447ba85d98d30c3857cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6823f3c012b749b2821c238fbbadc520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40fe6fcf396c4b8796ccee81e64fed72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e14dd9c4eed4d7aaee2becd3d53ee20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7be17be8db9a426bbaacfe49f056c8b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c39b6433aa4ec6bd72b6b3f04df4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}